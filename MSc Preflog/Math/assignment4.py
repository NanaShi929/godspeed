# -*- coding: utf-8 -*-
"""Assignment4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DlgzKuZ6izzmSX-DasTvQOntiSyuRI_D
"""

import pandas as pd
import numpy as np
import seaborn as sns

# Load Titanic dataset with >1000 records and mixed data types
df = sns.load_dataset('titanic').dropna()  # Drop missing values for simplicity

np.random.seed(42)

# ---------------------------
# Part A: Probability-Based Sampling Techniques
# ---------------------------

# 1. Simple Random Sampling (sample size ≈ 15% of dataset)
simple_random_sample = df.sample(frac=0.15, random_state=42)

# 2. Systematic Sampling (approximate 15% sample size)
step = len(df) // int(0.15 * len(df))
systematic_sample = df.iloc[::step].reset_index(drop=True)

# 3. Stratified Sampling based on 'sex'
stratified_sample = df.groupby('sex', group_keys=False).apply(lambda x: x.sample(frac=0.15, random_state=42)).reset_index(drop=True)

# 4. Cluster Sampling by selecting complete groups based on 'pclass'
selected_clusters = np.random.choice(df['pclass'].unique(), size=2, replace=False)
cluster_sample = df[df['pclass'].isin(selected_clusters)].reset_index(drop=True)

# ---------------------------
# Part B: Non-Probability-Based Sampling Techniques
# ---------------------------

# 5. Convenience Sampling (first 15% of records)
convenience_sample = df.head(int(0.15 * len(df)))

# 6. Purposive (Judgmental) Sampling - selecting passengers older than 50
purposive_sample = df[df['age'] > 50].reset_index(drop=True)

# 7. Snowball Sampling - expand from passengers in 'Southampton'
initial_group = df[df['embarked'] == 'S'].sample(5, random_state=42)
snowball_sample = initial_group.copy()
for _ in range(3):
    new_group = df[df['embarked'].isin(snowball_sample['embarked'])].sample(5, random_state=42)
    snowball_sample = pd.concat([snowball_sample, new_group]).drop_duplicates().reset_index(drop=True)

# 8. Quota Sampling - select up to 50 samples from each 'pclass'
quota_sample = df.groupby('pclass').apply(lambda x: x.sample(n=min(50, len(x)), random_state=42)).reset_index(drop=True)

# ---------------------------
# Output sample sizes for each technique
# ---------------------------

print("Sample sizes for each sampling technique:")
print(f"1. Simple Random Sampling: {len(simple_random_sample)}")
print(f"2. Systematic Sampling: {len(systematic_sample)}")
print(f"3. Stratified Sampling: {len(stratified_sample)}")
print(f"4. Cluster Sampling: {len(cluster_sample)}")
print(f"5. Convenience Sampling: {len(convenience_sample)}")
print(f"6. Purposive Sampling: {len(purposive_sample)}")
print(f"7. Snowball Sampling: {len(snowball_sample)}")
print(f"8. Quota Sampling: {len(quota_sample)}")

import pandas as pd
import numpy as np

# ---------------------------
# Load Titanic Dataset
# ---------------------------

# Adjust this path to where your CSV file is stored
file_path = '/content/Titanic-Dataset.csv'

# Load the dataset
df = pd.read_csv(file_path)

# Show the first 5 rows to verify
print("First 5 rows of the dataset:")
print(df.head())

# Drop rows with missing values for simplicity (optional)
df = df.dropna()

np.random.seed(42)

# ---------------------------
# Part A: Probability-Based Sampling Techniques
# ---------------------------

# 1. Simple Random Sampling (sample size ≈ 15% of dataset)
simple_random_sample = df.sample(frac=0.15, random_state=42)

# 2. Systematic Sampling (approximate 15% sample size)
step = len(df) // int(0.15 * len(df))
systematic_sample = df.iloc[::step].reset_index(drop=True)

# 3. Stratified Sampling based on 'Sex'
stratified_sample = pd.concat([
    group.sample(frac=0.15, random_state=42)
    for _, group in df.groupby('Sex')
]).reset_index(drop=True)

# 4. Cluster Sampling by selecting complete groups based on 'Pclass'
selected_clusters = np.random.choice(df['Pclass'].unique(), size=2, replace=False)
cluster_sample = df[df['Pclass'].isin(selected_clusters)].reset_index(drop=True)

# ---------------------------
# Part B: Non-Probability-Based Sampling Techniques
# ---------------------------

# 5. Convenience Sampling (first 15% of records)
convenience_sample = df.head(int(0.15 * len(df)))

# 6. Purposive (Judgmental) Sampling - selecting passengers older than 50
purposive_sample = df[df['Age'] > 50].reset_index(drop=True)

# 7. Snowball Sampling - expand from passengers in 'Embarked'
initial_group = df[df['Embarked'] == 'S'].sample(5, random_state=42)
snowball_sample = initial_group.copy()
for _ in range(3):
    new_group = df[df['Embarked'].isin(snowball_sample['Embarked'])].sample(5, random_state=42)
    snowball_sample = pd.concat([snowball_sample, new_group]).drop_duplicates().reset_index(drop=True)

# 8. Quota Sampling - select up to 50 samples from each 'Pclass'
quota_sample = pd.concat([
    group.sample(n=min(50, len(group)), random_state=42)
    for _, group in df.groupby('Pclass')
]).reset_index(drop=True)

# ---------------------------
# Output Sample Sizes
# ---------------------------

print("\nSample sizes for each sampling technique:")
print(f"1. Simple Random Sampling: {len(simple_random_sample)}")
print(f"2. Systematic Sampling: {len(systematic_sample)}")
print(f"3. Stratified Sampling: {len(stratified_sample)}")
print(f"4. Cluster Sampling: {len(cluster_sample)}")
print(f"5. Convenience Sampling: {len(convenience_sample)}")
print(f"6. Purposive Sampling: {len(purposive_sample)}")
print(f"7. Snowball Sampling: {len(snowball_sample)}")
print(f"8. Quota Sampling: {len(quota_sample)}")